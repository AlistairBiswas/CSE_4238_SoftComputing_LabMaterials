{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"z0tKD3f1Nt8y"},"source":["## MNIST Digit Recognizer (Neural Network)"]},{"cell_type":"markdown","metadata":{"id":"SyzBofWXmt0H"},"source":["\n","\n","\n","<div align=\"center\">\n","<img src=\"https://drive.google.com/uc?id=1VT-muG5HJoWaT9jwlmI6fe_7CjbW9x8I\" width=\"300\">\n","</div>\n","\n","\n","<div align=\"center\">\n","<img src=\"https://drive.google.com/uc?id=1foK0jI3dSuvCBBUbiqVKMiLn7x3ngA_x\" width=\"350\" height=\"200\">\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"7ILa4hbOxdnJ"},"source":["## One Layer FNN with Sigmoid Activation"]},{"cell_type":"code","metadata":{"id":"HJ1dVc9mgbN8"},"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torchvision.datasets as dsets"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o9vfh-4mtXt3"},"source":["<div align=\"center\">\n","<img src=\"https://drive.google.com/uc?id=16ZWsh6DrrwuzC4stYhsmcpIEGCke33Jc\" width=\"480\">\n","</div>\n"]},{"cell_type":"markdown","metadata":{"id":"EL0F4kOtWER5"},"source":[" - Our input size is determined by the size of the image **(height x width) = (28X28)**. Hence the size of our input is **784 (28 x 28)**.\n","\n"," - When we pass an image to our model, it will try to predict if it's **0, 1, 2, 3, 4, 5, 6, 7, 8, or 9**. That is a total of 10 classes, hence we have an output size of 10.\n","\n"," - Determining the **hidden layer size** is one of the crutial part. The first layer prior to the non-linear layer. This can be any **real number**. A large number of hidden nodes denotes a **bigger model with more parameters**.\n","\n","- The bigger model isn't **always the better model**. On the otner hand, bigger model requires **more training samples** to learn and converge to a good model.\n","\n","- Actually a bigger model **requires more training samples** to learn and converge to a good model. Hence, it is wise to pick the model size for the problem at hand. Because it is a simple problem of recognizing digits, we typically would not need a big model to achieve good results.\n","\n","- Moreover, too small of a hidden size would mean there would be **insufficient model capacity to predict competently**. Too small of a capacity denotes a **smaller brain capacity** so no matter how many training samples you provide, it has a maximum capacity boundary in terms of its **predictive power**."]},{"cell_type":"markdown","metadata":{"id":"fXVIydDCxDPS"},"source":["- **Input dimension:**\n","  - Size of image: $28 \\times 28 = 784$\n","\n","- **Output dimension: 10**\n","  - 0, 1, 2, 3, 4, 5, 6, 7, 8, 9"]},{"cell_type":"code","metadata":{"id":"o5hVijghPqz0"},"source":["# Hyperparameters\n","\n","batch_size = 100\n","num_iters = 3000\n","input_dim = 28*28 # num_features = 784\n","num_hidden = 100 # num of hidden nodes\n","output_dim = 10\n","\n","learning_rate = 0.1  # More power so we can learn faster! previously it was 0.001\n","\n","# Device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C4R6x4MvEsOT"},"source":["### Loading MNIST Dataset"]},{"cell_type":"code","metadata":{"id":"eUumuKA-cahD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f50aac08-00b1-42e3-d9bb-db6fb6dcda1d","executionInfo":{"status":"ok","timestamp":1701963081313,"user_tz":-360,"elapsed":971,"user":{"displayName":"Raihan Tanvir","userId":"13396016258112756152"}}},"source":["'''\n","LOADING DATASET\n","'''\n","train_dataset = dsets.MNIST(root='./data',\n","                            train=True,\n","                            transform=transforms.ToTensor(),  # Normalize the image to [0-1] from [0-255]\n","                            download=True)\n","\n","test_dataset = dsets.MNIST(root='./data',\n","                           train=False,\n","                           transform=transforms.ToTensor())\n","\n","'''\n","MAKING DATASET ITERABLE\n","'''\n","num_epochs = num_iters / (len(train_dataset) / batch_size)\n","num_epochs = int(num_epochs)\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True)   # It's better to shuffle the whole training dataset!\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 127003027.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 15787266.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 48442824.50it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 4746021.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n"]}]},{"cell_type":"code","metadata":{"id":"vmkMVvf8CLHf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4f9e6fe2-fe02-43b6-9be3-bf37072cd726","executionInfo":{"status":"ok","timestamp":1701963081313,"user_tz":-360,"elapsed":4,"user":{"displayName":"Raihan Tanvir","userId":"13396016258112756152"}}},"source":["print(len(train_dataset))\n","print(len(test_dataset))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["60000\n","10000\n"]}]},{"cell_type":"code","metadata":{"id":"Isz6lbl4Iovx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"aa2c9fc7-2401-4df5-b66e-53a585dfbad7","executionInfo":{"status":"ok","timestamp":1701963081313,"user_tz":-360,"elapsed":3,"user":{"displayName":"Raihan Tanvir","userId":"13396016258112756152"}}},"source":["# One Image Size\n","print(train_dataset[0][0].size())\n","print(train_dataset[0][0].numpy().shape)\n","# First Image Label\n","print(train_dataset[0][1])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 28, 28])\n","(1, 28, 28)\n","5\n"]}]},{"cell_type":"markdown","metadata":{"id":"GQB8tvNUQZop"},"source":["<div align=\"center\">\n","<img src=\"https://drive.google.com/uc?id=1mn8G92moF0MqXhD0J-M7cPidCYXR0hHS\" width=\"680\" height=\"380\">\n","</div>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nRm3MYkW8QVU"},"source":["### Step #1 : Design your model using class"]},{"cell_type":"code","metadata":{"id":"6mydzEXpeu7G"},"source":["class NeuralNetworkModel(nn.Module):\n","    def __init__(self, input_size, num_classes, num_hidden):\n","        super().__init__()\n","        ### 1st hidden layer\n","        self.linear_1 = nn.Linear(input_size, num_hidden)\n","\n","        ### Non-linearity\n","        self.sigmoid = nn.Sigmoid()\n","\n","        ### Output layer\n","        self.linear_out = nn.Linear(num_hidden, num_classes)\n","\n","    def forward(self, x):\n","        # Linear layer\n","        out  = self.linear_1(x)\n","        # Non-linearity\n","        out = self.sigmoid(out)\n","        # Linear layer (output)\n","        logits  = self.linear_out(out)\n","        return logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HIfiAaZB1rJz","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7323e447-56cf-41f4-9d87-57ef33ecefe7","executionInfo":{"status":"ok","timestamp":1701963088330,"user_tz":-360,"elapsed":6245,"user":{"displayName":"Raihan Tanvir","userId":"13396016258112756152"}}},"source":["'''\n","INSTANTIATE MODEL CLASS\n","'''\n","model = NeuralNetworkModel(input_size = input_dim,\n","                           num_classes = output_dim,\n","                           num_hidden = num_hidden)\n","# To enable GPU\n","model.to(device)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["NeuralNetworkModel(\n","  (linear_1): Linear(in_features=784, out_features=100, bias=True)\n","  (sigmoid): Sigmoid()\n","  (linear_out): Linear(in_features=100, out_features=10, bias=True)\n",")"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"pdrDJPOKzdSp"},"source":["###Step #2 : Construct loss and optimizer\n","\n","Unlike linear regression, we do not use MSE here, we need Cross Entropy Loss to calculate our loss before we backpropagate and update our parameters.\n","\n","`criterion = nn.CrossEntropyLoss() `\n","\n","It does 2 things at the same time.\n","\n","1. Computes softmax ([Logistic or Sigmoid]/softmax function)\n","2. Computes Cross Entropy Loss"]},{"cell_type":"code","metadata":{"id":"GM2q_XGHzcta"},"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I2Hb_JQ6AUok"},"source":["###Step #3 : Training: forward, loss, backward, step"]},{"cell_type":"code","metadata":{"id":"Q3Jb4vhRZI9p","colab":{"base_uri":"https://localhost:8080/"},"outputId":"37f721d2-3237-46a7-aac0-9215f5478585","executionInfo":{"status":"ok","timestamp":1701963133529,"user_tz":-360,"elapsed":45208,"user":{"displayName":"Raihan Tanvir","userId":"13396016258112756152"}}},"source":["'''\n","TRAIN THE MODEL\n","'''\n","iter = 0\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","\n","        images = images.view(-1, 28*28).to(device)\n","        labels = labels.to(device)\n","\n","        # Clear gradients w.r.t. parameters\n","        optimizer.zero_grad()\n","\n","        # Forward pass to get output/logits\n","        outputs = model(images)\n","\n","        # Calculate Loss: softmax --> cross entropy loss\n","        loss = criterion(outputs, labels)\n","\n","        # Getting gradients w.r.t. parameters\n","        loss.backward()\n","\n","        # Updating parameters\n","        optimizer.step()\n","\n","        iter += 1\n","\n","        if iter % 500 == 0:\n","            # Calculate Accuracy\n","            correct = 0\n","            total = 0\n","            # Iterate through test dataset\n","            for images, labels in test_loader:\n","\n","                images = images.view(-1, 28*28).to(device)\n","\n","                # Forward pass only to get logits/output\n","                outputs = model(images)\n","\n","                # Get predictions from the maximum value\n","                _, predicted = torch.max(outputs, 1)\n","\n","                # Total number of labels\n","                total += labels.size(0)\n","\n","\n","                # Total correct predictions\n","                if torch.cuda.is_available():\n","                    correct += (predicted.cpu() == labels.cpu()).sum()\n","                else:\n","                    correct += (predicted == labels).sum()\n","\n","            accuracy = 100 * correct.item() / total\n","\n","            # Print Loss\n","            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration: 500. Loss: 0.5977187752723694. Accuracy: 86.22\n","Iteration: 1000. Loss: 0.356582373380661. Accuracy: 89.53\n","Iteration: 1500. Loss: 0.38092106580734253. Accuracy: 90.43\n","Iteration: 2000. Loss: 0.374532550573349. Accuracy: 91.04\n","Iteration: 2500. Loss: 0.3183410167694092. Accuracy: 91.56\n","Iteration: 3000. Loss: 0.17307540774345398. Accuracy: 91.89\n"]}]},{"cell_type":"markdown","metadata":{"id":"t_5UaSvBJckA"},"source":["## Expanding Neural Network variants\n","\n","2 ways to expand a neural network\n","- Different non-linear activation\n","- More hidden layers"]},{"cell_type":"markdown","metadata":{"id":"sG1A_uEHL5uU"},"source":["## One Layer FNN with Tanh Activation"]},{"cell_type":"code","metadata":{"id":"cyXyrgHMw41l","colab":{"base_uri":"https://localhost:8080/"},"outputId":"34889770-bcee-4b28-aaa4-170f3f973063","executionInfo":{"status":"ok","timestamp":1701963174784,"user_tz":-360,"elapsed":41257,"user":{"displayName":"Raihan Tanvir","userId":"13396016258112756152"}}},"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torchvision.datasets as dsets\n","\n","# Hyperparameters\n","batch_size = 100\n","num_iters = 3000\n","input_dim = 28*28 # num_features = 784\n","num_hidden = 100\n","output_dim = 10\n","\n","learning_rate = 0.1\n","\n","# Device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","train_dataset = dsets.MNIST(root='./data',\n","                            train=True,\n","                            transform=transforms.ToTensor(),  # Normalize the image to [0-1] from [0-255]\n","                            download=True)\n","\n","test_dataset = dsets.MNIST(root='./data',\n","                           train=False,\n","                           transform=transforms.ToTensor())\n","\n","\n","num_epochs = num_iters / (len(train_dataset) / batch_size)\n","num_epochs = int(num_epochs)\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True)   # It's better to shuffle the whole training dataset!\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False)\n","\n","class NeuralNetworkModel(nn.Module):\n","    def __init__(self, input_size, num_classes, num_hidden):\n","        super().__init__()\n","        ### 1st hidden layer\n","        self.linear_1 = nn.Linear(input_size, num_hidden)\n","\n","        ### Non-linearity\n","        self.tanh = nn.Tanh()\n","\n","        ### Output layer\n","        self.linear_out = nn.Linear(num_hidden, num_classes)\n","\n","    def forward(self, x):\n","        # Linear layer\n","        out  = self.linear_1(x)\n","        # Non-linearity\n","        out = self.tanh(out)\n","        # Linear layer (output)\n","        probas  = self.linear_out(out)\n","        return probas\n","\n","model = NeuralNetworkModel(input_size = input_dim,\n","                           num_classes = output_dim,\n","                           num_hidden = num_hidden)\n","# To enable GPU\n","model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","iter = 0\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","\n","        images = images.view(-1, 28*28).to(device)\n","        labels = labels.to(device)\n","\n","        # Clear gradients w.r.t. parameters\n","        optimizer.zero_grad()\n","\n","        # Forward pass to get output/logits\n","        outputs = model(images)\n","\n","        # Calculate Loss: softmax --> cross entropy loss\n","        loss = criterion(outputs, labels)\n","\n","        # Getting gradients w.r.t. parameters\n","        loss.backward()\n","\n","        # Updating parameters\n","        optimizer.step()\n","\n","        iter += 1\n","\n","        if iter % 500 == 0:\n","            # Calculate Accuracy\n","            correct = 0\n","            total = 0\n","            # Iterate through test dataset\n","            for images, labels in test_loader:\n","\n","                images = images.view(-1, 28*28).to(device)\n","\n","                # Forward pass only to get logits/output\n","                outputs = model(images)\n","\n","                # Get predictions from the maximum value\n","                _, predicted = torch.max(outputs, 1)\n","\n","                # Total number of labels\n","                total += labels.size(0)\n","\n","\n","                # Total correct predictions\n","                if torch.cuda.is_available():\n","                    correct += (predicted.cpu() == labels.cpu()).sum()\n","                else:\n","                    correct += (predicted == labels).sum()\n","\n","            accuracy = 100 * correct.item() / total\n","\n","            # Print Loss\n","            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration: 500. Loss: 0.2507927417755127. Accuracy: 91.09\n","Iteration: 1000. Loss: 0.3160650432109833. Accuracy: 92.49\n","Iteration: 1500. Loss: 0.28000736236572266. Accuracy: 93.38\n","Iteration: 2000. Loss: 0.11709506809711456. Accuracy: 94.0\n","Iteration: 2500. Loss: 0.3231379985809326. Accuracy: 94.44\n","Iteration: 3000. Loss: 0.1393723487854004. Accuracy: 95.08\n"]}]},{"cell_type":"markdown","metadata":{"id":"hoxfWq6ZNPiL"},"source":["## One Layer FNN with ReLU Activation"]},{"cell_type":"code","metadata":{"id":"PGVrecUPMsyT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"eaa5b788-239a-4b0d-a705-d9bd15fde645","executionInfo":{"status":"ok","timestamp":1701963216868,"user_tz":-360,"elapsed":42087,"user":{"displayName":"Raihan Tanvir","userId":"13396016258112756152"}}},"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torchvision.datasets as dsets\n","\n","# Hyperparameters\n","batch_size = 100\n","num_iters = 3000\n","input_dim = 28*28 # num_features = 784\n","num_hidden = 100\n","output_dim = 10\n","\n","learning_rate = 0.1\n","\n","# Device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","train_dataset = dsets.MNIST(root='./data',\n","                            train=True,\n","                            transform=transforms.ToTensor(),  # Normalize the image to [0-1] from [0-255]\n","                            download=True)\n","\n","test_dataset = dsets.MNIST(root='./data',\n","                           train=False,\n","                           transform=transforms.ToTensor())\n","\n","\n","num_epochs = num_iters / (len(train_dataset) / batch_size)\n","num_epochs = int(num_epochs)\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True)   # It's better to shuffle the whole training dataset!\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False)\n","\n","class NeuralNetworkModel(nn.Module):\n","    def __init__(self, input_size, num_classes, num_hidden):\n","        super().__init__()\n","        ### 1st hidden layer\n","        self.linear_1 = nn.Linear(input_size, num_hidden)\n","\n","        ### Non-linearity\n","        self.relu = nn.ReLU()\n","\n","        ### Output layer\n","        self.linear_out = nn.Linear(num_hidden, num_classes)\n","\n","    def forward(self, x):\n","        # Linear layer\n","        out  = self.linear_1(x)\n","        # Non-linearity\n","        out = self.relu(out)\n","        # Linear layer (output)\n","        probas  = self.linear_out(out)\n","        return probas\n","\n","model = NeuralNetworkModel(input_size = input_dim,\n","                           num_classes = output_dim,\n","                           num_hidden = num_hidden)\n","# To enable GPU\n","model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","iter = 0\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","\n","        images = images.view(-1, 28*28).to(device)\n","        labels = labels.to(device)\n","\n","        # Clear gradients w.r.t. parameters\n","        optimizer.zero_grad()\n","\n","        # Forward pass to get output/logits\n","        outputs = model(images)\n","\n","        # Calculate Loss: softmax --> cross entropy loss\n","        loss = criterion(outputs, labels)\n","\n","        # Getting gradients w.r.t. parameters\n","        loss.backward()\n","\n","        # Updating parameters\n","        optimizer.step()\n","\n","        iter += 1\n","\n","        if iter % 500 == 0:\n","            # Calculate Accuracy\n","            correct = 0\n","            total = 0\n","            # Iterate through test dataset\n","            for images, labels in test_loader:\n","\n","                images = images.view(-1, 28*28).to(device)\n","\n","                # Forward pass only to get logits/output\n","                outputs = model(images)\n","\n","                # Get predictions from the maximum value\n","                _, predicted = torch.max(outputs, 1)\n","\n","                # Total number of labels\n","                total += labels.size(0)\n","\n","\n","                # Total correct predictions\n","                if torch.cuda.is_available():\n","                    correct += (predicted.cpu() == labels.cpu()).sum()\n","                else:\n","                    correct += (predicted == labels).sum()\n","\n","            accuracy = 100 * correct.item() / total\n","\n","            # Print Loss\n","            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration: 500. Loss: 0.3028669059276581. Accuracy: 91.4\n","Iteration: 1000. Loss: 0.21906547248363495. Accuracy: 93.05\n","Iteration: 1500. Loss: 0.1611238270998001. Accuracy: 94.09\n","Iteration: 2000. Loss: 0.19845980405807495. Accuracy: 94.73\n","Iteration: 2500. Loss: 0.16394956409931183. Accuracy: 95.24\n","Iteration: 3000. Loss: 0.07317963242530823. Accuracy: 95.79\n"]}]},{"cell_type":"markdown","metadata":{"id":"T_r9Lq-O0FJi"},"source":["## Two Layer FNN with ReLU Activation"]},{"cell_type":"code","metadata":{"id":"YNhAoGbnNasI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4d01a259-4b95-41a5-acbf-0ab939efd44d","executionInfo":{"status":"ok","timestamp":1701963262637,"user_tz":-360,"elapsed":45772,"user":{"displayName":"Raihan Tanvir","userId":"13396016258112756152"}}},"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torchvision.datasets as dsets\n","\n","# Hyperparameters\n","batch_size = 100\n","num_iters = 3000\n","input_dim = 28*28 # num_features = 784\n","num_hidden = 100\n","output_dim = 10\n","\n","learning_rate = 0.1\n","\n","# Device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","train_dataset = dsets.MNIST(root='./data',\n","                            train=True,\n","                            transform=transforms.ToTensor(),  # Normalize the image to [0-1] from [0-255]\n","                            download=True)\n","\n","test_dataset = dsets.MNIST(root='./data',\n","                           train=False,\n","                           transform=transforms.ToTensor())\n","\n","\n","num_epochs = num_iters / (len(train_dataset) / batch_size)\n","num_epochs = int(num_epochs)\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True)   # It's better to shuffle the whole training dataset!\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False)\n","\n","class DeepNeuralNetworkModel(nn.Module):\n","    def __init__(self, input_size, num_classes, num_hidden):\n","        super().__init__()\n","        ### 1st hidden layer: 784 --> 100\n","        self.linear_1 = nn.Linear(input_size, num_hidden)\n","        ### Non-linearity in 1st hidden layer\n","        self.relu_1 = nn.ReLU()\n","\n","        ### 2nd hidden layer: 100 --> 100\n","        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n","        ### Non-linearity in 2nd hidden layer\n","        self.relu_2 = nn.ReLU()\n","\n","        ### Output layer: 100 --> 10\n","        self.linear_out = nn.Linear(num_hidden, num_classes)\n","\n","    def forward(self, x):\n","        ### 1st hidden layer\n","        out  = self.linear_1(x)\n","        ### Non-linearity in 1st hidden layer\n","        out = self.relu_1(out)\n","\n","        ### 2nd hidden layer\n","        out  = self.linear_2(out)\n","        ### Non-linearity in 2nd hidden layer\n","        out = self.relu_2(out)\n","\n","        # Linear layer (output)\n","        probas  = self.linear_out(out)\n","        return probas\n","\n","# INSTANTIATE MODEL CLASS\n","\n","model = DeepNeuralNetworkModel(input_size = input_dim,\n","                               num_classes = output_dim,\n","                               num_hidden = num_hidden)\n","# To enable GPU\n","model.to(device)\n","\n","# INSTANTIATE LOSS & OPTIMIZER CLASS\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","iter = 0\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","\n","        images = images.view(-1, 28*28).to(device)\n","        labels = labels.to(device)\n","\n","        # Clear gradients w.r.t. parameters\n","        optimizer.zero_grad()\n","\n","        # Forward pass to get output/logits\n","        outputs = model(images)\n","\n","        # Calculate Loss: softmax --> cross entropy loss\n","        loss = criterion(outputs, labels)\n","\n","        # Getting gradients w.r.t. parameters\n","        loss.backward()\n","\n","        # Updating parameters\n","        optimizer.step()\n","\n","        iter += 1\n","\n","        if iter % 500 == 0:\n","            # Calculate Accuracy\n","            correct = 0\n","            total = 0\n","            # Iterate through test dataset\n","            for images, labels in test_loader:\n","\n","                images = images.view(-1, 28*28).to(device)\n","\n","                # Forward pass only to get logits/output\n","                outputs = model(images)\n","\n","                # Get predictions from the maximum value\n","                _, predicted = torch.max(outputs, 1)\n","\n","                # Total number of labels\n","                total += labels.size(0)\n","\n","\n","                # Total correct predictions\n","                if torch.cuda.is_available():\n","                    correct += (predicted.cpu() == labels.cpu()).sum()\n","                else:\n","                    correct += (predicted == labels).sum()\n","\n","            accuracy = 100 * correct.item() / total\n","\n","            # Print Loss\n","            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration: 500. Loss: 0.2984124422073364. Accuracy: 91.3\n","Iteration: 1000. Loss: 0.35222136974334717. Accuracy: 93.6\n","Iteration: 1500. Loss: 0.14257687330245972. Accuracy: 94.83\n","Iteration: 2000. Loss: 0.18339788913726807. Accuracy: 95.49\n","Iteration: 2500. Loss: 0.0796729251742363. Accuracy: 96.17\n","Iteration: 3000. Loss: 0.1587733030319214. Accuracy: 96.62\n"]}]},{"cell_type":"markdown","metadata":{"id":"JYdiRLt3FPLy"},"source":["## Three Layer FNN with ReLU Activation"]},{"cell_type":"code","metadata":{"id":"T0jY7KZ0E50C","colab":{"base_uri":"https://localhost:8080/"},"outputId":"40d0d5fb-642a-4d30-afdf-ddb8f4204d92","executionInfo":{"status":"ok","timestamp":1701963306093,"user_tz":-360,"elapsed":43466,"user":{"displayName":"Raihan Tanvir","userId":"13396016258112756152"}}},"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torchvision.datasets as dsets\n","\n","# Hyperparameters\n","batch_size = 100\n","num_iters = 3000\n","input_dim = 28*28 #num_features = 784\n","num_hidden = 100\n","output_dim = 10\n","\n","learning_rate = 0.1\n","\n","# Device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","train_dataset = dsets.MNIST(root='./data',\n","                            train=True,\n","                            transform=transforms.ToTensor(),  # Normalize the image to [0-1] from [0-255]\n","                            download=True)\n","\n","test_dataset = dsets.MNIST(root='./data',\n","                           train=False,\n","                           transform=transforms.ToTensor())\n","\n","\n","num_epochs = num_iters / (len(train_dataset) / batch_size)\n","num_epochs = int(num_epochs)\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True)   # It's better to shuffle the whole training dataset!\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False)\n","\n","class DeepNeuralNetworkModel(nn.Module):\n","    def __init__(self, input_size, num_classes, num_hidden):\n","        super().__init__()\n","        ### 1st hidden layer: 784 --> 100\n","        self.linear_1 = nn.Linear(input_size, num_hidden)\n","        ### Non-linearity in 1st hidden layer\n","        self.relu_1 = nn.ReLU()\n","\n","        ### 2nd hidden layer: 100 --> 100\n","        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n","        ### Non-linearity in 2nd hidden layer\n","        self.relu_2 = nn.ReLU()\n","\n","        ### 3rd hidden layer: 100 --> 100\n","        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n","        ### Non-linearity in 3rd hidden layer\n","        self.relu_3 = nn.ReLU()\n","\n","        ### Output layer: 100 --> 10\n","        self.linear_out = nn.Linear(num_hidden, num_classes)\n","\n","    def forward(self, x):\n","        ### 1st hidden layer\n","        out  = self.linear_1(x)\n","        ### Non-linearity in 1st hidden layer\n","        out = self.relu_1(out)\n","\n","        ### 2nd hidden layer\n","        out  = self.linear_2(out)\n","        ### Non-linearity in 2nd hidden layer\n","        out = self.relu_2(out)\n","\n","        ### 3rd hidden layer\n","        out  = self.linear_3(out)\n","        ### Non-linearity in 3rd hidden layer\n","        out = self.relu_3(out)\n","\n","        # Linear layer (output)\n","        probas  = self.linear_out(out)\n","        return probas\n","\n","# INSTANTIATE MODEL CLASS\n","\n","model = DeepNeuralNetworkModel(input_size = input_dim,\n","                               num_classes = output_dim,\n","                               num_hidden = num_hidden)\n","# To enable GPU\n","model.to(device)\n","\n","# INSTANTIATE LOSS & OPTIMIZER CLASS\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","iter = 0\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","\n","        images = images.view(-1, 28*28).to(device)\n","        labels = labels.to(device)\n","\n","        # Clear gradients w.r.t. parameters\n","        optimizer.zero_grad()\n","\n","        # Forward pass to get output/logits\n","        outputs = model(images)\n","\n","        # Calculate Loss: softmax --> cross entropy loss\n","        loss = criterion(outputs, labels)\n","\n","        # Getting gradients w.r.t. parameters\n","        loss.backward()\n","\n","        # Updating parameters\n","        optimizer.step()\n","\n","        iter += 1\n","\n","        if iter % 500 == 0:\n","            # Calculate Accuracy\n","            correct = 0\n","            total = 0\n","            # Iterate through test dataset\n","            for images, labels in test_loader:\n","\n","                images = images.view(-1, 28*28).to(device)\n","\n","                # Forward pass only to get logits/output\n","                outputs = model(images)\n","\n","                # Get predictions from the maximum value\n","                _, predicted = torch.max(outputs, 1)\n","\n","                # Total number of labels\n","                total += labels.size(0)\n","\n","\n","                # Total correct predictions\n","                if torch.cuda.is_available():\n","                    correct += (predicted.cpu() == labels.cpu()).sum()\n","                else:\n","                    correct += (predicted == labels).sum()\n","\n","            accuracy = 100 * correct.item() / total\n","\n","            # Print Loss\n","            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration: 500. Loss: 0.357700377702713. Accuracy: 90.29\n","Iteration: 1000. Loss: 0.3482294976711273. Accuracy: 93.36\n","Iteration: 1500. Loss: 0.15070495009422302. Accuracy: 95.29\n","Iteration: 2000. Loss: 0.10639841854572296. Accuracy: 96.2\n","Iteration: 2500. Loss: 0.1744196116924286. Accuracy: 96.31\n","Iteration: 3000. Loss: 0.012470705434679985. Accuracy: 96.86\n"]}]},{"cell_type":"markdown","metadata":{"id":"-1vTvfpkNjHL"},"source":["## What's Next?\n","\n","- Try with other activations from Pytorch.\n","- Try different activations for different layers (We used ReLU Only)\n","- Try adding more hidden layers\n","- Try increasing the hidden layer neurons (We used 100 here in this example)\n","- Try experimenting with different neurons for different hidden layers (We here in this examples used a fixed sixe: 100)\n","\n","\n","<div align=\"center\">\n","<img src=\"https://drive.google.com/uc?id=1VYlYjGEYo6JKsiADnzOCNM2TPkhNI-Yq\" width=\"230\" height=\"580\">\n","</div>\n","\n","<div align=\"center\">\n","<img src=\"https://drive.google.com/uc?id=1hMrKBdhQ8cmhxGgCzFczQi4xpHMsHufD\" width=\"680\" height=\"280\">\n","</div>\n","\n"]}]}