{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CjQDebxMtjG"
      },
      "source": [
        "## Natural Language Processing (NLP) - A hands-on introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuPcELhlLCJB"
      },
      "source": [
        "### Popular Libraries\n",
        "\n",
        "- [NLTK](https://www.nltk.org/)\n",
        "- [spaCy ](https://spacy.io/)\n",
        "\n",
        "**NLTK & spaCy** is a free open-source library for Natural Language Processing (NLP) in Python to support teaching, research, and development. Which are:-\n",
        "  - Free and Open source\n",
        "  - Easy to use\n",
        "  - Modular\n",
        "  - Well documented\n",
        "  - Simple and extensible\n",
        "\n",
        "In this notebook, I will provide basic NLP tasks that we need in order to process raw text to find useful informations. For each tasks, we will be using NLTK as well as spaCy. Good news is that both are installed in Google Colab by default."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvxtJdF1RYUT"
      },
      "source": [
        "### Some definitions\n",
        "\n",
        "- **Corpus** - Corpora is the plural of Corpus. **\"Corpus\"** mainly appears in NLP area or application domain related to texts/documents, because of its meaning \"a collection of written texts\"\n",
        "    - **Example:** A collection of news documents.\n",
        "\n",
        "- **Dataset** - dataset appears in every application domain (in can be **image/video/text/numerical/mixed**) --- a collection of any kind of data is a dataset.\n",
        "\n",
        "- **Lexicon** - vocabulary or list of Words and their meanings.\n",
        "    - **Example:** English dictionary.\n",
        "\n",
        "- **Token** - Each \"entity\" that is a part of whatever was split up based on\n",
        "rules.\n",
        "    - For examples, each word is a token when a sentence is \"tokenized\" into\n",
        "words. Each sentence can also be a token, if you tokenized the sentences out\n",
        "of a paragraph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZFH748nqbZ8"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "Tokenization is the process of breaking a stream of text up into sentences, words, phrases, symbols, or other meaningful elements called tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2E3OsfqlqqBl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab061051-3ee9-4e36-d186-8107f06e5e6c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# For tokenizing words and sentences\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "s = \"Good muffins cost $3.88\\nin New York. Please buy me two of them.\\n\\nThanks.\"\n",
        "\n",
        "print (sent_tokenize(s))\n",
        "print (word_tokenize(s))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Good muffins cost $3.88\\nin New York.', 'Please buy me two of them.', 'Thanks.']\n",
            "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B49lU_b1XGN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d533a87-36f0-497c-ca9e-7d6d0e0ac498"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Small spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(\"Good muffins cost $3.88\\nin New York. Please buy me two of them.\\n\\nThanks.\")\n",
        "\n",
        "print(\"\\n\\nTokenized Sentences\")\n",
        "\n",
        "for i, sent in enumerate(doc.sents):\n",
        "        print('-->Sentence %d: %s' % (i, sent.text))\n",
        "\n",
        "print(\"\\n\\nTokenized Words\")\n",
        "\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Tokenized Sentences\n",
            "-->Sentence 0: Good muffins cost $3.88\n",
            "in New York.\n",
            "-->Sentence 1: Please buy me two of them.\n",
            "\n",
            "\n",
            "-->Sentence 2: Thanks.\n",
            "\n",
            "\n",
            "Tokenized Words\n",
            "['Good', 'muffins', 'cost', '$', '3.88', '\\n', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', '\\n\\n', 'Thanks', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt-7RAu-8WoL"
      },
      "source": [
        "### Downloading Large spaCy model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zACJ-rtS6A-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b0c5fac-3991-4c7d-c9c8-8ed38d94979a"
      },
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "\n",
        "import en_core_web_lg\n",
        "\n",
        "nlp = en_core_web_lg.load()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-01-30 07:43:41.145527: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-lg==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.4.1/en_core_web_lg-3.4.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-lg==3.4.1) (3.4.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (8.1.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (21.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.10.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.11)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.0.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.1)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.4.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIMjiA98_6Km"
      },
      "source": [
        "## Filtering stopwords\n",
        "\n",
        " - **Stopwords** are common words that **generally** do not contribute to the\n",
        "meaning of a sentence.\n",
        " - Most search engines will filter stopwords out of search queries and\n",
        "documents in order to **save space and time** in their index.\n",
        "- Removing stopwords is not a hard and fast rule in NLP. It depends upon the task that we are working on.\n",
        "- For tasks like text classification, where the text is to be classified into different categories, stopwords are removed or excluded from the given text so that more focus can be given to those words which define the meaning of the text.\n",
        " - All [Stopwords](https://github.com/stopwords-iso/stopwords-iso) collection including Bengali."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_M90K-HApNi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6780c8b4-7fea-4ac2-aa46-45fb0fe9c27e"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# All english stopwords list\n",
        "english_stops = set(stopwords.words('english'))\n",
        "\n",
        "print (english_stops)\n",
        "\n",
        "words = ['The', 'natural', 'language', 'processing', 'is', 'very', 'interesting']\n",
        "filtered_words = [word for word in words if word.lower() not in english_stops]    # word.lower() is for lowering down the words\n",
        "\n",
        "print(filtered_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"haven't\", 'where', 'out', 'the', 'mightn', 'down', 'shouldn', 'should', \"hadn't\", 'needn', 'other', 'did', 'few', 'he', 'themselves', 'by', 'and', 'her', 'in', 'are', 'wasn', 'our', 'you', 'then', \"hasn't\", 'most', \"couldn't\", 'ma', 'itself', 'during', \"she's\", 'these', 'it', 'd', \"mightn't\", 'haven', 'who', \"isn't\", 'your', 'a', 'herself', \"you'll\", 'don', \"you're\", 'against', 'be', 'doing', 'had', \"shouldn't\", 'over', 'so', 'theirs', 'of', 'very', 'me', 'each', 'she', 'nor', 'yourselves', 'some', 'only', 'won', 'this', 'shan', 'do', 'from', 'if', 'is', 'ourselves', 'being', 'up', 'll', 'an', 'been', 'y', 'how', 'hasn', 'couldn', 'weren', 'ours', 'before', 'myself', 'yourself', 'will', 'to', 'why', 'hers', 'were', 'above', 'after', 're', 'same', 'mustn', 'with', 'into', 'hadn', 'am', 'my', \"wouldn't\", 'once', \"doesn't\", 'because', \"weren't\", 'on', \"you've\", 'more', 'they', 'isn', 'aren', 'didn', 'ain', 'when', 'him', 'further', 'below', 'does', 'at', 'doesn', 'its', \"shan't\", 'whom', 'which', 'yours', 'all', 'as', 'just', 've', 'm', 'wouldn', 'them', 'there', 'now', 'has', \"needn't\", 'again', 'what', 'here', 'but', 'about', \"should've\", \"that'll\", 'his', \"won't\", \"it's\", 'that', 'o', \"wasn't\", 'i', 'having', \"you'd\", 'under', 'not', 'own', \"don't\", \"aren't\", \"mustn't\", 't', 'those', 'we', 'both', 'no', 'their', 'through', 'for', 'between', 'until', 'was', 'himself', 'such', 'than', 's', 'or', 'any', \"didn't\", 'can', 'too', 'while', 'have', 'off'}\n",
            "['natural', 'language', 'processing', 'interesting']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1oeIOcgsWsv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "681dd2b5-86ce-416b-ca01-0bcf9c31d926"
      },
      "source": [
        "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "print('Number of stop words: %d' % len(spacy_stopwords))\n",
        "print('First ten stop words: %s' % list(spacy_stopwords)[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of stop words: 326\n",
            "First ten stop words: ['hers', 'various', 'nobody', 'who', 'after', 'until', 'per', 'least', 'there', 'us']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7F0TnPuqLNs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d49bd42-7269-42e9-fadf-1b198beb03a3"
      },
      "source": [
        "doc = nlp(\"Good muffins cost $3.88\\nin New York. Please buy me two of them.\\n\\nThanks.\")\n",
        "\n",
        "tokens = [token.text for token in doc if not token.is_stop]\n",
        "\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Good', 'muffins', 'cost', '$', '3.88', '\\n', 'New', 'York', '.', 'buy', '.', '\\n\\n', 'Thanks', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpJM8xKR1ebv"
      },
      "source": [
        "## Adding Custom Stopwords\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tw3p0Sc81sTy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "f48867c5-9e2e-450b-f495-5ff30fbdc519"
      },
      "source": [
        "\n",
        "english_stops = set(stopwords.words('english'))\n",
        "\n",
        "print (english_stops)\n",
        "\n",
        "english_stops.remove('is')\n",
        "english_stops.add('natural')\n",
        "\n",
        "\n",
        "words = ['The', 'natural', 'language', 'processing', 'is', 'very', 'interesting']\n",
        "filtered_words = [word for word in words if word.lower() not in english_stops]\n",
        "\n",
        "print(filtered_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'hers', 'wouldn', 'who', 'after', 'until', 'd', 'there', 'in', 'don', 'about', 'now', 'the', 'out', 't', 'below', \"mustn't\", 'what', 'by', \"couldn't\", 'both', 'have', 'his', 'didn', 'has', 'won', 'ours', 'it', 'you', \"mightn't\", 'is', 'too', 'weren', \"isn't\", 'through', 'needn', 'during', 'a', 'most', 'i', \"should've\", 'she', 'an', 'they', 'her', 'other', \"aren't\", 've', 'was', 'from', \"it's\", 'or', \"weren't\", 'ourselves', \"shouldn't\", \"wasn't\", 'yourselves', 'couldn', 'down', 'll', 'were', 'those', 'had', 'him', 'did', \"wouldn't\", 'wasn', 'where', 'can', 'which', 'be', \"needn't\", 'are', 'o', 'once', \"that'll\", 'if', 'above', 'should', 'into', 'why', 'y', 'herself', 'to', \"you'd\", 'off', 'our', 'my', 's', 'ma', 'myself', \"you'll\", 'when', 'm', 'themselves', 'at', 'yourself', 'under', 'over', 'this', 'before', 'again', 'very', 'yours', 'all', 'own', 'on', 'as', \"won't\", 'mightn', 'mustn', 'me', 'here', 'them', 'these', 'your', 'because', 'himself', 'having', 'shouldn', \"doesn't\", 'with', 'nor', 'hadn', \"you've\", 'doing', 'but', 'of', 'doesn', 'and', 'each', 'while', 'ain', 'any', 'theirs', 'aren', 'we', 'just', 'how', 'itself', \"hadn't\", 'he', 'than', 'for', \"didn't\", 'not', 'few', 'some', 'shan', \"you're\", 'hasn', 're', 'only', \"hasn't\", 'haven', 'its', \"don't\", 'do', 'further', 'whom', \"haven't\", 'then', 'their', \"shan't\", 'no', 'does', 'isn', 'am', 'been', 'against', 'will', 'same', 'more', \"she's\", 'that', 'such', 'so', 'up', 'being', 'between'}\n",
            "['language', 'processing', 'is', 'interesting']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQC1aGc6G148"
      },
      "source": [
        "## Edit Distance\n",
        "\n",
        "The edit distance is the number of character changes necessary to\n",
        "transform the given word into the suggested word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuWv7OGZG5rH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7e5d3b0e-a9f0-4d3e-e140-4303b21abda3"
      },
      "source": [
        "from nltk.metrics import edit_distance\n",
        "\n",
        "print(edit_distance(\"Birthday\",\"Bday\"))\n",
        "\n",
        "print(edit_distance(\"university\", \"varsity\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyXhKAnChBtP"
      },
      "source": [
        "## Removing Punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSByg19ehLyk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "e9d49530-a7f6-4941-82c6-4a26f8c4c7b9"
      },
      "source": [
        "import string\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "puncset = list(string.punctuation)\n",
        "\n",
        "sentence = \"Hun Sen's Cambodian can't People's Party won 64 of the 122 parliamentary seats in party July's elections, short of the two-thirds majority needed to form a government on its own.\"\n",
        "\n",
        "sentence = sentence.lower()\n",
        "print(sentence)\n",
        "sentence = nltk.word_tokenize(sentence)\n",
        "print(sentence)\n",
        "sentence = [i for i in sentence if i not in puncset] # Removing punctuation\n",
        "print(sentence)\n",
        "sentence = [w for w in sentence if w.isalpha()] # Removing numbers and punctuation\n",
        "print(sentence)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "hun sen's cambodian can't people's party won 64 of the 122 parliamentary seats in party july's elections, short of the two-thirds majority needed to form a government on its own.\n",
            "['hun', 'sen', \"'s\", 'cambodian', 'ca', \"n't\", 'people', \"'s\", 'party', 'won', '64', 'of', 'the', '122', 'parliamentary', 'seats', 'in', 'party', 'july', \"'s\", 'elections', ',', 'short', 'of', 'the', 'two-thirds', 'majority', 'needed', 'to', 'form', 'a', 'government', 'on', 'its', 'own', '.']\n",
            "['hun', 'sen', \"'s\", 'cambodian', 'ca', \"n't\", 'people', \"'s\", 'party', 'won', '64', 'of', 'the', '122', 'parliamentary', 'seats', 'in', 'party', 'july', \"'s\", 'elections', 'short', 'of', 'the', 'two-thirds', 'majority', 'needed', 'to', 'form', 'a', 'government', 'on', 'its', 'own']\n",
            "['hun', 'sen', 'cambodian', 'ca', 'people', 'party', 'won', 'of', 'the', 'parliamentary', 'seats', 'in', 'party', 'july', 'elections', 'short', 'of', 'the', 'majority', 'needed', 'to', 'form', 'a', 'government', 'on', 'its', 'own']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V_sSlmJHrxy"
      },
      "source": [
        "## Normalizing Text\n",
        "\n",
        "The goal of both stemming and lemmatization is to **\"normalize\"** words\n",
        "to their **common base form**, which is useful for many text-processing\n",
        "applications.\n",
        "\n",
        " - **Stemming** = heuristically removing the affixes of a word, to get its\n",
        "**stem (root)**.\n",
        "    - It is a rule-based process of stripping the suffixes **(“ing”, “ly”, “es”, “s” etc)** from a word\n",
        " - **Lemmatization** = Lemmatization process involves first determining\n",
        "the part of speech of a word, and applying different normalization\n",
        "rules for each part of speech.\n",
        "\n",
        "Consider:\n",
        " - I was taking a **ride** in the car.\n",
        " - I was **riding** in the car.\n",
        "\n",
        "Imagine every word in the English language, every possible tense and affix you\n",
        "can put on a word. **Having individual dictionary entries per version would be highly redundant and inefficient.**\n",
        "\n",
        "- Lisa **ate** the food and washed the dishes.\n",
        "- They were **eating** noodles at a cafe.\n",
        "- Don’t you want to **eat** before we leave?\n",
        "- We have just **eaten** our breakfast.\n",
        "- It also **eats** fruit and vegetables.\n",
        "\n",
        "Unfortunately, that is not the case with machines. **They treat these words differently**. Therefore, we need to normalize them to their root word, which is **“eat”** in our example.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NiiSnW4PNds"
      },
      "source": [
        "### Stemming\n",
        "\n",
        " - One of the **most popular** stemming algorithms is the Porter stemmer,\n",
        "which has been around since 1979.\n",
        " - Several other stemming algorithms provided by NLTK are Lancaster\n",
        "Stemmer and Snowball Stemmer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzHkTXgpPNEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a48491f5-8b21-47ba-82f1-5a48bdd6c1ab"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
        "\n",
        "for w in example_words:\n",
        "  print(stemmer.stem(w))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python\n",
            "python\n",
            "python\n",
            "python\n",
            "pythonli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3E8K8EvUuFR"
      },
      "source": [
        "## Lemmatization\n",
        "\n",
        "Lemmatize takes a part of speech parameter, \"pos.\" **If not supplied,\n",
        "the default is \"noun\".**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ot6yhd_UxrD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fc54907-2f36-40f9-c28b-e368d434c67d"
      },
      "source": [
        "## Lemmatization using NLTK\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download(\"omw-1.4\")\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(lemmatizer.lemmatize('cooking'))\n",
        "print(lemmatizer.lemmatize('cooking', pos='v'))  # noun = n, verb = v, ajdective = a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cooking\n",
            "cook\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnky8ws26MZM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a3a00b7-9709-4b8e-a22c-91f091d60928"
      },
      "source": [
        "## Lemmatization using spaCy\n",
        "\n",
        "doc = nlp('Jim bought 300 shares of Acme Corp. in 2006.')\n",
        "\n",
        "lemma_words = []\n",
        "\n",
        "for token in doc:\n",
        "    lemma_words.append(token.lemma_)\n",
        "\n",
        "print(lemma_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Jim', 'buy', '300', 'share', 'of', 'Acme', 'Corp.', 'in', '2006', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuZUzIuvlcW3"
      },
      "source": [
        "## Comparison between stemming and lemmatizing\n",
        "\n",
        "The major difference between these is, as you saw earlier, **stemming\n",
        "can often create non-existent words**, whereas **lemmas are actual\n",
        "words**, you can just look up in an English dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPANLhcx-Ul9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "07379cb3-0d77-49c4-bac4-494e31bb7374"
      },
      "source": [
        "print(stemmer.stem('believes'))\n",
        "print(lemmatizer.lemmatize('believes'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "believ\n",
            "belief\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQp_RdbclyuZ"
      },
      "source": [
        "## Part-of-speech Tagging\n",
        "\n",
        "The English language is formed of different parts of speech (POS) like nouns, verbs, pronouns, adjectives, etc. POS tagging analyzes the words in a sentences and associates it with a POS tag depending on the way it is used.\n",
        "\n",
        "Full [tag list](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBd2CLajmxor"
      },
      "source": [
        "## Penn Bank Part-of-Speech Tags\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://drive.google.com/uc?id=18MqGTRZcK3jYd5Ix8BOaODE-6SGcn_CC\" width=\"700\" height=\"380\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ysrGI9Ell3i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "eb11c78e-3544-4412-b4e6-5f755109efdf"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "words = word_tokenize('Jim bought 300 shares of Acme Corp. in 2006.')\n",
        "\n",
        "tagged_words = pos_tag(words)\n",
        "\n",
        "print(tagged_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[('Jim', 'NNP'), ('bought', 'VBD'), ('300', 'CD'), ('shares', 'NNS'), ('of', 'IN'), ('Acme', 'NNP'), ('Corp.', 'NNP'), ('in', 'IN'), ('2006', 'CD'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78iSj2027LTU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "3b1225fc-0314-4184-897e-db766e992971"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp('Jim bought 300 shares of Acme Corp. in 2006.')\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.tag_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Jim PROPN NNP\n",
            "bought VERB VBD\n",
            "300 NUM CD\n",
            "shares NOUN NNS\n",
            "of ADP IN\n",
            "Acme PROPN NNP\n",
            "Corp. PROPN NNP\n",
            "in ADP IN\n",
            "2006 NUM CD\n",
            ". PUNCT .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W36xzFWEooo0"
      },
      "source": [
        "## Named-entity Recognition\n",
        "\n",
        "Named-entity recognition is a subtask of information extraction that\n",
        "seeks to locate and classify elements in text into pre-defined\n",
        "categories such as the names of **persons**, **organizations**, **locations**,\n",
        "**expressions of times**, **quantities**, **monetary values**, **percentages**, etc.\n",
        "\n",
        "**NE Type and Examples:-**\n",
        "\n",
        " - **ORGANIZATION** - Georgia-Pacific Corp., WHO\n",
        " - **PERSON** - Eddy Bonte, President Obama\n",
        " - **LOCATION** - Murray River, Mount Everest\n",
        " - **DATE**- June, 2008-06-29\n",
        " - **TIME** - two fifty a m, 1:30 p.m.\n",
        " - **MONEY** - 175 million Canadian Dollars, GBP 10.40\n",
        " - **PERCENT** - twenty pct, 18.75 %\n",
        " - **FACILITY** - Washington Monument, Stonehenge\n",
        " - **GPE** - South East Asia, Midlothian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPRLKi8-mCt4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "fd0a0fc1-32e1-49d0-82f7-6ac557921def"
      },
      "source": [
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "sent = 'Jim bought 300 shares of Acme Corp. in 2006.'\n",
        "\n",
        "print(ne_chunk(pos_tag(wordpunct_tokenize(sent))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "(S\n",
            "  (PERSON Jim/NNP)\n",
            "  bought/VBD\n",
            "  300/CD\n",
            "  shares/NNS\n",
            "  of/IN\n",
            "  (ORGANIZATION Acme/NNP Corp/NNP)\n",
            "  ./.\n",
            "  in/IN\n",
            "  2006/CD\n",
            "  ./.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UB3RkUwpVBU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b6a198f8-12a8-465a-83f2-9f537979b028"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Jim bought 300 shares of Acme Corp. in 2006.\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Jim 0 3 PERSON\n",
            "300 11 14 CARDINAL\n",
            "Acme Corp. 25 35 ORG\n",
            "2006 39 43 DATE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrAvitXRveJb"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}